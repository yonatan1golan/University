{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521c3f33",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3fa7f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.final_project_venv (Python 3.9.6)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/yonatangolan/Desktop/github/University/final_project/.final_project_venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# reddit\n",
    "import praw\n",
    "\n",
    "# nlp\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# graph\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# config\n",
    "from secrets_config import RedditSecretsConfig\n",
    "\n",
    "# system\n",
    "from datetime import datetime, timezone\n",
    "from collections import defaultdict, Counter\n",
    "from pytz import UTC\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import emoji\n",
    "import csv\n",
    "import ast\n",
    "import re\n",
    "\n",
    "import site\n",
    "site.addsitedir(\"NLP\")\n",
    "\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "import unicodedata\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765c855",
   "metadata": {},
   "source": [
    "## CONGIF Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41855a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    INDIVIDUALS = [\n",
    "        \"Elon Musk\",           # CEO of Tesla. Founder, product architect, and central to all decisions at Tesla.\n",
    "        \"Jeff Bezos\",          # Amazon founder. Business and space rival to Musk (Blue Origin vs. SpaceX); often compared to Musk.\n",
    "        \"Larry Page\",          # Google co-founder. Longtime friend of Musk; Google/Waymo competes with Tesla in autonomous driving.\n",
    "        \"Donald Trump\",        # U.S. President. Influences public opinion and policy; has made comments on Tesla and Musk.\n",
    "        \"Mark Zuckerberg\",     # Meta CEO. Competes with Musk in social media; has made comments on Tesla and Musk.\n",
    "        \"Joe Biden\",           # U.S. President. Initially dismissive of Tesla in EV discussions; later acknowledged its EV leadership.\n",
    "        \"Cathie Wood\",         # CEO of ARK Invest. Major Tesla bull and investor; forecasts extremely high valuations for Tesla.\n",
    "        \"Jim Cramer\",          # CNBC host. Publicly flip-flopped on Tesla; currently supportive but controversial in Tesla circles.\n",
    "        \"Chamath Palihapitiya\",# VC and SPAC investor. Public Tesla bull and Musk supporter; promoted Tesla on media.\n",
    "        \"Michael Burry\",       # Famed for The Big Short. Publicly shorted Tesla; skeptical of valuation.\n",
    "        \"Gavin Newsom\",        # Governor of California. Tesla's home state; has made comments on Tesla and Musk.\n",
    "        \"Alexandria Ocasio-Cortez\", # U.S. Congresswoman. Criticized Musk and Tesla on social issues; represents a younger, progressive demographic.\n",
    "        \"Pete Buttigieg\",      # U.S. Secretary of Transportation. Has commented on Tesla's role in EV adoption and infrastructure.\n",
    "        \"Bernie Sanders\",      # U.S. Senator. Criticized Musk for wealth and influence; represents a progressive viewpoint on wealth inequality.\n",
    "    ]\n",
    "\n",
    "    SUBREDDITS = [\n",
    "        'TeslaMotors',         # Main Tesla discussion hub\n",
    "        'TeslaInvestorsClub',  # Tesla investment focused  \n",
    "        'wallstreetbets',      # Retail trading community\n",
    "        'investing',           # General investment discussions\n",
    "        'electricvehicles',    # General EV discussions\n",
    "        'technology',          # General tech discussions\n",
    "        'politics',            # U.S. political discussions\n",
    "        'RealTesla',           # Critical Tesla perspectives\n",
    "        'elonmusk'             # Elon Musk specific\n",
    "    ]\n",
    "\n",
    "    ALIASES = {\n",
    "        \"Elon Musk\": [\"Elon\", \"Musk\", \"ElonMusk\", \"SpaceX\", \"X.com\", \"Tesla CEO\"],\n",
    "        \"Jeff Bezos\": [\"Bezos\", \"Jeff\", \"Amazon founder\", \"Blue Origin\", \"JB\"],\n",
    "        \"Mark Zuckerberg\": [\"Zuck\", \"Zuckerberg\", \"Meta CEO\", \"Facebook\"],\n",
    "        \"Larry Page\": [\"Larry\", \"Google co-founder\", \"Alphabet\"],\n",
    "        \"Donald Trump\": [\"Trump\", \"Donald\", \"POTUS 45\", \"45th President\", \"The Donald\"],\n",
    "        \"Joe Biden\": [\"Biden\", \"President Biden\", \"Joe\"],\n",
    "        \"Cathie Wood\": [\"Cathie\", \"ARK\", \"ARK Invest\", \"Cathie W\", \"ARKK\"],\n",
    "        \"Jim Cramer\": [\"Cramer\", \"Mad Money\", \"Jim\", \"CNBC host\"],\n",
    "        \"Chamath Palihapitiya\": [\"Chamath\", \"Chamath P\", \"Social Capital\", \"SPAC King\"],\n",
    "        \"Michael Burry\": [\"Burry\", \"The Big Short\", \"Dr. Burry\", \"Scion Capital\"],\n",
    "        \"Gavin Newsom\": [\"Newsom\", \"Governor Newsom\", \"CA Governor\"],\n",
    "        \"Alexandria Ocasio-Cortez\": [\"AOC\", \"Ocasio-Cortez\", \"Congresswoman AOC\"],\n",
    "        \"Pete Buttigieg\": [\"Buttigieg\", \"Mayor Pete\", \"Transportation Secretary\"],\n",
    "        \"Bernie Sanders\": [\"Bernie\", \"Senator Sanders\", \"Sanders\"],\n",
    "    }\n",
    "\n",
    "    COMPARATIVE_COMPANIES = [\n",
    "        \"Rivian\", \"NIO\", \"Lucid\", \"BYD\", \"Ford\", \"GM\", \"Apple\", \"Meta\", \"Palantir\"\n",
    "    ]\n",
    "\n",
    "    TIME_PERIODS = {\n",
    "        \"<2020\": (\"2010-01-01\", \"2019-12-31\"),\n",
    "        \"2020-2021\": (\"2020-01-01\", \"2021-12-31\"),\n",
    "        \"2022-2023\": (\"2022-01-01\", \"2023-12-31\"),\n",
    "        \"2024-2025\": (\"2024-01-01\", \"2025-12-31\"),\n",
    "    }\n",
    "\n",
    "    POST_LIMIT = 50\n",
    "    COMMENT_LIMIT = 20\n",
    "    SENTIMENT_THRESHOLD = 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5ac27",
   "metadata": {},
   "source": [
    "## Reddit Client Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc82e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditClient:\n",
    "    def __init__(self):\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=RedditSecretsConfig.client_id,\n",
    "            client_secret=RedditSecretsConfig.client_secret,\n",
    "            user_agent=RedditSecretsConfig.user_agent\n",
    "        )\n",
    "        self.all_posts = []\n",
    "\n",
    "    def _fetch_top_posts(self, subreddit_name, keyword, post_limit=CONFIG.POST_LIMIT):\n",
    "        subreddit = self.reddit.subreddit(subreddit_name)\n",
    "        posts = []\n",
    "\n",
    "        for post in subreddit.search(keyword, limit=post_limit, sort='top'):\n",
    "            post_info = {\n",
    "                'title': post.title,\n",
    "                'created_utc': datetime.fromtimestamp(post.created_utc, tz=timezone.utc),\n",
    "                'score': post.score,\n",
    "                'num_comments': post.num_comments,\n",
    "                'comments': self._fetch_top_comments(post, CONFIG.COMMENT_LIMIT),\n",
    "            }\n",
    "            posts.append(post_info)\n",
    "        return posts\n",
    "\n",
    "    def _fetch_top_comments(self, post, comment_limit):\n",
    "        post.comments.replace_more(limit=0)\n",
    "        top_comments = post.comments[:comment_limit]\n",
    "        return [\n",
    "            {\n",
    "                'author': str(comment.author),\n",
    "                'body': comment.body,\n",
    "                'score': comment.score\n",
    "            }\n",
    "            for comment in top_comments if comment.body not in ['[deleted]', '[removed]']\n",
    "        ]\n",
    "\n",
    "    def fetch_all_posts(self):\n",
    "        for individual in CONFIG.INDIVIDUALS:\n",
    "            print(f\"Fetching posts for {individual}...\")\n",
    "            for subreddit in CONFIG.SUBREDDITS:\n",
    "                posts = self._fetch_top_posts(subreddit, individual)\n",
    "                self.all_posts.extend(posts)\n",
    "            pd.DataFrame(posts).to_csv(f\"posts_data/{individual.lower().replace(' ', '_')}.csv\", index=False)\n",
    "\n",
    "    def get_all_posts(self):\n",
    "        \"\"\" returns all posts from all individuals, for the saved local files \"\"\"\n",
    "        to_return = []\n",
    "        for individual in CONFIG.INDIVIDUALS:\n",
    "            posts = pd.read_csv(f\"posts_data/{individual.lower().replace(' ', '_')}.csv\")\n",
    "            to_return.extend(posts.to_dict(orient='records'))\n",
    "        for post in to_return:\n",
    "            post['comments'] = ast.literal_eval(post['comments'])\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a134785",
   "metadata": {},
   "source": [
    "## Processor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eb799d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self):\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def _analyze_sentiment(self, text):\n",
    "        score = self.sia.polarity_scores(text)['compound']\n",
    "        sentiment = 'positive' if score >= CONFIG.SENTIMENT_THRESHOLD else 'negative' if score <= -CONFIG.SENTIMENT_THRESHOLD else 'neutral'\n",
    "        return score, sentiment\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        text = emoji.demojize(text, delimiters=(\"\", \"\"))\n",
    "        text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '', text)\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "        text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "        text = re.sub(r'[^\\w\\s\\.,!?\\'\"\\-]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        doc = nlp(text)\n",
    "        text = ' '.join([token.lemma_ for token in doc])\n",
    "        return text\n",
    "\n",
    "    def _assign_post_periods(self, posts):\n",
    "        copied = posts.copy()\n",
    "        for post in copied:\n",
    "            post['periods'] = []\n",
    "            for period_name, (start_str, end_str) in CONFIG.TIME_PERIODS.items():\n",
    "                start = pd.to_datetime(start_str).tz_localize(UTC)\n",
    "                end = pd.to_datetime(end_str).tz_localize(UTC)\n",
    "                created_utc = pd.to_datetime(post['created_utc'])\n",
    "                if start <= created_utc <= end:\n",
    "                    post['periods'].append(period_name)\n",
    "                    break\n",
    "        return copied\n",
    "\n",
    "    def process_posts(self, posts):\n",
    "        with_periods = self._assign_post_periods(posts)\n",
    "        for post in with_periods:\n",
    "            post['title'] = self.normalize_text(post['title'])\n",
    "            score, label = self._analyze_sentiment(post['title'])\n",
    "            post['sentiment_score'] = score\n",
    "            post['sentiment_label'] = label\n",
    "            for comment in post['comments']:\n",
    "                comment['body'] = self.normalize_text(comment['body'])\n",
    "                score, label = self._analyze_sentiment(comment['body'])\n",
    "                comment['sentiment_score'] = score\n",
    "                comment['sentiment_label'] = label\n",
    "        return with_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f185ecc",
   "metadata": {},
   "source": [
    "## Graph Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d8ce39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMaker:\n",
    "    def __init__(self, given_period: str = None):\n",
    "        self.graph = nx.Graph()\n",
    "        self.entities = [\"Elon Musk\"] + CONFIG.INDIVIDUALS\n",
    "        self.period = given_period\n",
    "        self.min_width = 0.5\n",
    "        self.max_width = 10.0\n",
    "        self.custom_cmap = LinearSegmentedColormap.from_list(\n",
    "            \"custom_red_gray_green\",\n",
    "            [\"#f80509\", \"#deb603\", \"#0bc746\"],\n",
    "            N=256\n",
    "        )\n",
    "        self.node_sentiment = defaultdict(list)\n",
    "        self.competitor_mentions = defaultdict(int)\n",
    "\n",
    "    def _find_mentions(self, text):\n",
    "        mentions = set()\n",
    "        lowered = text.lower()\n",
    "        for entity in self.entities:\n",
    "            aliases = [entity] + CONFIG.ALIASES.get(entity, [])\n",
    "            for alias in aliases:\n",
    "                if alias.lower() in lowered:\n",
    "                    mentions.add(entity)\n",
    "\n",
    "        for comp in CONFIG.COMPARATIVE_COMPANIES:\n",
    "            if comp.lower() in lowered:\n",
    "                mentions.add(comp)\n",
    "                self.competitor_mentions[comp] += 1\n",
    "\n",
    "        return list(mentions)\n",
    "\n",
    "    def build_graph(self, posts):\n",
    "        for post in posts:\n",
    "            mentions = self._find_mentions(post['title'])\n",
    "            self._add_edges(mentions, post['score'], post['sentiment_score'])\n",
    "            for mention in mentions:\n",
    "                self.node_sentiment[mention].append(post['sentiment_score'])\n",
    "            for comment in post['comments']:\n",
    "                comment_mentions = self._find_mentions(comment['body'])\n",
    "                self._add_edges(comment_mentions, comment['score'], comment['sentiment_score'])\n",
    "                for mention in comment_mentions:\n",
    "                    self.node_sentiment[mention].append(comment['sentiment_score'])\n",
    "\n",
    "    def _add_edges(self, mentions, score, sentiment_score):\n",
    "        for i in range(len(mentions)):\n",
    "            for j in range(i + 1, len(mentions)):\n",
    "                a, b = mentions[i], mentions[j]\n",
    "                if self.graph.has_edge(a, b):\n",
    "                    self.graph[a][b]['weight'] += score\n",
    "                    self.graph[a][b]['sentiments'].append(sentiment_score)\n",
    "                else:\n",
    "                    self.graph.add_edge(a, b, weight=score, sentiments=[sentiment_score])\n",
    "\n",
    "    def finalize_graph(self):\n",
    "        weights = [data['weight'] for _, _, data in self.graph.edges(data=True)]\n",
    "        min_w = min(weights) if weights else 1\n",
    "        max_w = max(weights) if weights else 1\n",
    "\n",
    "        for u, v, data in self.graph.edges(data=True):\n",
    "            if max_w != min_w:\n",
    "                norm_weight = self.min_width + (data['weight'] - min_w) / (max_w - min_w) * (self.max_width - self.min_width)\n",
    "            else:\n",
    "                norm_weight = (self.max_width + self.min_width) / 2\n",
    "            avg_sentiment = sum(data['sentiments']) / len(data['sentiments'])\n",
    "            data['normalized_weight'] = norm_weight\n",
    "            data['color'] = self._sentiment_to_color(avg_sentiment)\n",
    "\n",
    "    def _sentiment_to_color(self, score):\n",
    "        norm = mcolors.Normalize(vmin=-1.0, vmax=1.0)\n",
    "        rgba = self.custom_cmap(norm(score))\n",
    "        return rgba\n",
    "\n",
    "    def _get_node_sentiment_color(self, node):\n",
    "        if node in self.node_sentiment and self.node_sentiment[node]:\n",
    "            avg_sentiment = sum(self.node_sentiment[node]) / len(self.node_sentiment[node])\n",
    "            return self._sentiment_to_color(avg_sentiment)\n",
    "        else:\n",
    "            return self._sentiment_to_color(0.0)\n",
    "\n",
    "    def print_neighborhood(self):\n",
    "        for node in self.graph.nodes:\n",
    "            neighbors = list(self.graph.neighbors(node))\n",
    "            print(f\"Neighbors of {node}:\")\n",
    "            for neighbor in neighbors:\n",
    "                weight = self.graph[node][neighbor]['weight']\n",
    "                sentiment = sum(self.graph[node][neighbor]['sentiments']) / len(self.graph[node][neighbor]['sentiments'])\n",
    "                print(f\"  {neighbor}: weight={weight}, sentiment={sentiment}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "    def save_graph_info(self):\n",
    "        def top_n(dictionary, n=3):\n",
    "            return sorted(dictionary.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "        path = f\"graphs_analysis/{self.period.lower().replace(' ', '_')}.txt\"\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(\"ðŸ“Š Graph Overview:\\n\")\n",
    "            f.write(\"ðŸ”¹ Basic Stats:\\n\")\n",
    "            f.write(f\"  Nodes: {len(self.graph.nodes)}\\n\")\n",
    "            f.write(f\"  Edges: {len(self.graph.edges)}\\n\")\n",
    "            f.write(f\"  Is Connected: {self.is_graph_connected()}\\n\")\n",
    "            f.write(f\"  Density: {self.get_graph_density():.4f}\\n\")\n",
    "            f.write(f\"  Diameter: {self.get_graph_diameter()}\\n\")\n",
    "            f.write(f\"  Average Degree: {self.get_graph_average_degree():.2f}\\n\")\n",
    "            f.write(f\"  Avg Clustering Coefficient: {self.get_graph_average_clustering():.4f}\\n\")\n",
    "            f.write(f\"  Avg Shortest Path Length: {self.get_graph_average_shortest_path_length():.2f}\\n\")\n",
    "            f.write(f\"  Avg Node Sentiment: {self.get_graph_average_node_sentiment():.2f}\\n\")\n",
    "\n",
    "            f.write(\"\\nðŸ”¹ Structural Analysis:\\n\")\n",
    "            cut_vertices = self.get_cut_vertexes()\n",
    "            bridges = self.get_bridges()\n",
    "            f.write(f\"  Cut Vertices (Articulation Points): {cut_vertices if cut_vertices else 'None'}\\n\")\n",
    "            f.write(f\"  Bridges (Critical Edges): {bridges if bridges else 'None'}\\n\")\n",
    "\n",
    "            f.write(\"\\nðŸ”¹ Community Detection:\\n\")\n",
    "            communities = self.get_communities()\n",
    "            f.write(f\"  Number of Communities: {len(communities)}\\n\")\n",
    "            community_sizes = [len(c) for c in communities]\n",
    "            f.write(f\"  Community Sizes: {community_sizes}\\n\")\n",
    "\n",
    "            f.write(\"\\nðŸ”¹ Centrality Measures (Top 3 Nodes per Metric):\\n\")\n",
    "            centrality = self.get_centrality_measures()\n",
    "            for metric, values in centrality.items():\n",
    "                top_nodes = top_n(values)\n",
    "                top_str = \", \".join(f\"{node} ({score:.2f})\" for node, score in top_nodes)\n",
    "                f.write(f\"  {metric.capitalize()}: {top_str}\\n\")\n",
    "\n",
    "            f.write(f\"\\nðŸ”¹ Node Sentiments:\\n\")\n",
    "            for node in self.graph.nodes:\n",
    "                if node in self.node_sentiment and self.node_sentiment[node]:\n",
    "                    avg_sentiment = sum(self.node_sentiment[node]) / len(self.node_sentiment[node])\n",
    "                    f.write(f\"  {node}: {avg_sentiment:.3f} (from {len(self.node_sentiment[node])} mentions)\\n\")\n",
    "                else:\n",
    "                    f.write(f\"  {node}: No sentiment data\\n\")\n",
    "\n",
    "            f.write(f\"\\nðŸ”¹ Structure Analysis:\\n {self.get_community_structure()}\")\n",
    "\n",
    "    def get_cut_vertexes(self):\n",
    "        return list(nx.articulation_points(self.graph))\n",
    "\n",
    "    def get_bridges(self):\n",
    "        return list(nx.bridges(self.graph))\n",
    "\n",
    "    def get_communities(self):\n",
    "        comp = nx.algorithms.community.girvan_newman(self.graph)\n",
    "        for _ in range(1):\n",
    "            communities = next(comp)\n",
    "        return list(communities)\n",
    "\n",
    "    def get_community_structure(self, verbose=True):\n",
    "        communities = self.get_communities()\n",
    "        community_structure = {i: list(community) for i, community in enumerate(communities)}\n",
    "        community_summary = \"\"\n",
    "        if verbose:\n",
    "            for i, members in community_structure.items():\n",
    "                community_summary += f\"Community {i}: {', '.join(members)}\\n\"\n",
    "                community_summary += f\"  Size: {len(members)}\\n\"\n",
    "                community_summary += f\"  Members: {', '.join(members)}\\n\\n\"\n",
    "        return community_structure\n",
    "\n",
    "    def get_centrality_measures(self):\n",
    "        return {\n",
    "            \"degree\": nx.degree_centrality(self.graph),\n",
    "            \"betweenness\": nx.betweenness_centrality(self.graph),\n",
    "            \"closeness\": nx.closeness_centrality(self.graph)\n",
    "        }\n",
    "\n",
    "    def get_graph_diameter(self):\n",
    "        if nx.is_connected(self.graph):\n",
    "            return nx.diameter(self.graph)\n",
    "        else:\n",
    "            return max(\n",
    "                max(lengths.values())\n",
    "                for node, lengths in nx.single_source_shortest_path_length(self.graph).items()\n",
    "            )\n",
    "\n",
    "    def get_graph_density(self):\n",
    "        return nx.density(self.graph)\n",
    "\n",
    "    def get_graph_average_clustering(self):\n",
    "        return nx.average_clustering(self.graph)\n",
    "\n",
    "    def get_graph_average_shortest_path_length(self):\n",
    "        if nx.is_connected(self.graph):\n",
    "            return nx.average_shortest_path_length(self.graph)\n",
    "        else:\n",
    "            return float('inf')\n",
    "\n",
    "    def get_graph_average_degree(self):\n",
    "        return sum(dict(self.graph.degree()).values()) / len(self.graph.nodes) if self.graph.nodes else 0\n",
    "\n",
    "    def get_graph_average_node_sentiment(self):\n",
    "        sentiments = [sum(scores) / len(scores) for scores in self.node_sentiment.values() if scores]\n",
    "        return sum(sentiments) / len(sentiments) if sentiments else 0\n",
    "\n",
    "    def is_graph_connected(self):\n",
    "        return nx.is_connected(self.graph)\n",
    "\n",
    "    def get_edge_sentiment_analysis(self, sentiment_thresholds=(-0.1, 0.1)):\n",
    "        edge_insights = []\n",
    "        for u, v, data in self.graph.edges(data=True):\n",
    "            sentiments = data['sentiments']\n",
    "            avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0\n",
    "            relation = (\n",
    "                \"Negative\" if avg_sentiment < sentiment_thresholds[0]\n",
    "                else \"Positive\" if avg_sentiment > sentiment_thresholds[1]\n",
    "                else \"Neutral\"\n",
    "            )\n",
    "            edge_insights.append({\n",
    "                \"node_1\": u,\n",
    "                \"node_2\": v,\n",
    "                \"average_sentiment\": round(avg_sentiment, 3),\n",
    "                \"relation_type\": relation,\n",
    "                \"num_mentions\": len(sentiments),\n",
    "                \"total_weight\": data['weight']\n",
    "            })\n",
    "        return sorted(edge_insights, key=lambda x: abs(x[\"average_sentiment\"]), reverse=True)\n",
    "\n",
    "    def export_edge_sentiment_analysis(self, filepath=\"edge_sentiment_analysis.csv\"):\n",
    "        analysis = self.get_edge_sentiment_analysis()\n",
    "        with open(filepath, mode='w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=analysis[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(analysis)\n",
    "\n",
    "    def visualize(self):\n",
    "        fig, ax = plt.subplots(figsize=(18, 14))\n",
    "\n",
    "        central_node = [\"Elon Musk\"]\n",
    "        influencer_nodes = [n for n in self.graph.nodes if n in CONFIG.INDIVIDUALS and n != \"Elon Musk\"]\n",
    "        competitor_nodes = [n for n in self.graph.nodes if n in CONFIG.COMPARATIVE_COMPANIES]\n",
    "        other_nodes = [n for n in self.graph.nodes if n not in central_node + influencer_nodes + competitor_nodes]\n",
    "\n",
    "        shells = [central_node, influencer_nodes, competitor_nodes + other_nodes]\n",
    "        pos = nx.shell_layout(self.graph, shells)\n",
    "\n",
    "        edge_colors = [data['color'] for _, _, data in self.graph.edges(data=True)]\n",
    "        edge_weights = [data['normalized_weight'] for _, _, data in self.graph.edges(data=True)]\n",
    "        node_colors = [self._get_node_sentiment_color(node) for node in self.graph.nodes]\n",
    "\n",
    "        nx.draw_networkx_nodes(self.graph, pos, node_color=node_colors, node_size=1300, alpha=0.9, ax=ax)\n",
    "        nx.draw_networkx_labels(self.graph, pos, font_size=10, ax=ax)\n",
    "        nx.draw_networkx_edges(self.graph, pos, edge_color=edge_colors, width=edge_weights, alpha=0.6, ax=ax)\n",
    "\n",
    "        sm = plt.cm.ScalarMappable(cmap=self.custom_cmap, norm=plt.Normalize(vmin=-1, vmax=1))\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, ax=ax, shrink=0.8, aspect=20)\n",
    "        cbar.set_label('Sentiment Score', rotation=270, labelpad=15)\n",
    "\n",
    "        ax.set_title(f\"Elon Musk & Influencers for period {self.period} (Node colors based on sentiment)\", fontsize=16)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f43e4",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a423f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextAnalysis:\n",
    "    def __init__(self, posts, period=None, aliases=None, min_word_freq=4, max_words=200, ngram_range=(1, 1)):\n",
    "        self.posts = posts\n",
    "        self.period = period\n",
    "        self.min_word_freq = min_word_freq\n",
    "        self.max_words = max_words\n",
    "        self.ngram_range = ngram_range\n",
    "        self.averaged_scores = {}\n",
    "        self.word_counts = Counter()\n",
    "        self.word_contexts = defaultdict(list)\n",
    "        self.aliases = aliases or {}\n",
    "        self.stop_words = stopwords.words('english')\n",
    "\n",
    "        self.alias_lookup = {}\n",
    "        for canonical, alias_list in self.aliases.items():\n",
    "            for alias in alias_list:\n",
    "                self.alias_lookup[alias.lower()] = canonical.lower()\n",
    "\n",
    "    def _color_func(self, word, **kwargs):\n",
    "        score = self.averaged_scores.get(word.lower(), 0)\n",
    "        try:\n",
    "            rgba = GraphMaker(self.period)._sentiment_to_color(score)\n",
    "            r, g, b, _ = [int(c * 255) for c in rgba]\n",
    "            return f\"rgb({r}, {g}, {b})\"\n",
    "        except:\n",
    "            if score > 0.1:\n",
    "                intensity = min(int(255 * abs(score)), 255)\n",
    "                return f\"rgb(0, {intensity}, 0)\"\n",
    "            elif score < -0.1:\n",
    "                intensity = min(int(255 * abs(score)), 255)\n",
    "                return f\"rgb({intensity}, 0, 0)\"\n",
    "            else:\n",
    "                return \"rgb(128, 128, 128)\"\n",
    "\n",
    "    def _replace_aliases(self, text):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        text_lower = text.lower()\n",
    "        for alias, canonical in self.alias_lookup.items():\n",
    "            canonical_token = canonical.replace(\" \", \"_\")\n",
    "            pattern = re.compile(rf'\\b{re.escape(alias)}\\b', re.IGNORECASE)\n",
    "            text_lower = pattern.sub(canonical_token, text_lower)\n",
    "        return text_lower\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        tokens = re.findall(r'\\b[a-z_]+\\b', text.lower())\n",
    "        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
    "\n",
    "        ngram_tokens = []\n",
    "        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):\n",
    "            ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "            ngram_tokens.extend(['_'.join(ngram) for ngram in ngrams])\n",
    "\n",
    "        return ngram_tokens\n",
    "\n",
    "    def process_all_text(self):\n",
    "        word_scores = defaultdict(list)\n",
    "        seen_contexts = set()\n",
    "\n",
    "        for post in self.posts:\n",
    "            if post.get('title'):\n",
    "                sentiment = post.get('sentiment_score', 0)\n",
    "                title_text = self._replace_aliases(post['title'])\n",
    "                tokenized = self._tokenize(title_text)\n",
    "                for word in set(tokenized):\n",
    "                    self.word_counts[word] += tokenized.count(word)\n",
    "                    word_scores[word].append(sentiment)\n",
    "                    key = (word, post['title'])\n",
    "                    if key not in seen_contexts:\n",
    "                        self.word_contexts[word].append((post['title'], sentiment))\n",
    "                        seen_contexts.add(key)\n",
    "\n",
    "            for comment in post.get('comments', []):\n",
    "                if comment.get('body'):\n",
    "                    sentiment = comment.get('sentiment_score', 0)\n",
    "                    body_text = self._replace_aliases(comment['body'])\n",
    "                    tokenized = self._tokenize(body_text)\n",
    "                    for word in set(tokenized):\n",
    "                        self.word_counts[word] += tokenized.count(word)\n",
    "                        word_scores[word].append(sentiment)\n",
    "                        key = (word, comment['body'])\n",
    "                        if key not in seen_contexts:\n",
    "                            self.word_contexts[word].append((comment['body'], sentiment))\n",
    "                            seen_contexts.add(key)\n",
    "\n",
    "        self.averaged_scores = {\n",
    "            word: np.mean(scores) for word, scores in word_scores.items()\n",
    "        }\n",
    "\n",
    "    def explain_word_sentiment(self, word, limit=3):\n",
    "        print(f\"\\nExplanation for word '{word}'\")\n",
    "        contexts = self.word_contexts.get(word, [])\n",
    "        if not contexts:\n",
    "            print(\"  No context found.\")\n",
    "            return\n",
    "\n",
    "        total_score = 0\n",
    "        for i, (text, score) in enumerate(contexts[:limit]):\n",
    "            print(f\"  [{i+1}] Sentiment: {score:.3f} | Text: {text[:100]}...\")\n",
    "            total_score += score\n",
    "\n",
    "        avg = self.averaged_scores.get(word, 0)\n",
    "        print(f\"  âž¤ Averaged Sentiment: {avg:.3f} from {min(limit, len(contexts))} samples\")\n",
    "\n",
    "    def get_filtered_word_frequencies(self):\n",
    "        return {\n",
    "            word: count for word, count in self.word_counts.items()\n",
    "            if count >= self.min_word_freq\n",
    "        }\n",
    "\n",
    "    def get_most_common_words(self, n=20):\n",
    "        filtered_counts = self.get_filtered_word_frequencies()\n",
    "        return Counter(filtered_counts).most_common(n)\n",
    "\n",
    "    def get_sentiment_extremes(self, n=10):\n",
    "        eligible_words = {\n",
    "            word: score for word, score in self.averaged_scores.items()\n",
    "            if self.word_counts[word] >= self.min_word_freq\n",
    "        }\n",
    "\n",
    "        sorted_by_sentiment = sorted(eligible_words.items(), key=lambda x: x[1])\n",
    "        most_negative = sorted_by_sentiment[:n]\n",
    "        most_positive = sorted_by_sentiment[-n:]\n",
    "\n",
    "        return {\n",
    "            'most_negative': most_negative,\n",
    "            'most_positive': most_positive\n",
    "        }\n",
    "\n",
    "    def generate_word_cloud(self, save_path=None, words=None, word_list=None):\n",
    "        if word_list:\n",
    "            words = Counter(word_list)\n",
    "        elif words is None:\n",
    "            filtered_frequencies = self.get_filtered_word_frequencies()\n",
    "            words = dict(Counter(filtered_frequencies).most_common(self.max_words))\n",
    "\n",
    "        if not words:\n",
    "            print(\"No words available to generate the word cloud.\")\n",
    "            return\n",
    "\n",
    "        wordcloud = WordCloud(\n",
    "            width=800,\n",
    "            height=400,\n",
    "            background_color='white',\n",
    "            max_words=self.max_words,\n",
    "            relative_scaling=0.5,\n",
    "            colormap='viridis'\n",
    "        ).generate_from_frequencies(words)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud.recolor(color_func=self._color_func), interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        title = f\"Word Cloud Colored by Sentiment\"\n",
    "        if self.period:\n",
    "            title += f\" for {self.period}\"\n",
    "        plt.title(title, fontsize=16, pad=20)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def run_lsa(self, num_topics=5):\n",
    "        texts = []\n",
    "\n",
    "        for post in self.posts:\n",
    "            if post.get('title'):\n",
    "                title_tokens = self._tokenize(self._replace_aliases(post['title']))\n",
    "                if title_tokens:\n",
    "                    texts.append(title_tokens)\n",
    "\n",
    "            for comment in post.get('comments', []):\n",
    "                if comment.get('body'):\n",
    "                    body_tokens = self._tokenize(self._replace_aliases(comment['body']))\n",
    "                    if body_tokens:\n",
    "                        texts.append(body_tokens)\n",
    "\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "        if not corpus:\n",
    "            print(\"No valid documents for LSA after filtering.\")\n",
    "            return None\n",
    "\n",
    "        lsa = models.LsiModel(corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        topics = lsa.print_topics(num_topics=num_topics, num_words=8)\n",
    "\n",
    "        print(f\"\\nLSA Topics (num_topics={num_topics}):\")\n",
    "        print(\"=\" * 50)\n",
    "        for i, topic in enumerate(topics):\n",
    "            print(f\"Topic {i}: {topic[1]}\")\n",
    "\n",
    "        try:\n",
    "            coherence_model = CoherenceModel(\n",
    "                model=lsa, texts=texts, dictionary=dictionary, coherence='c_v'\n",
    "            )\n",
    "            coherence_score = coherence_model.get_coherence()\n",
    "            print(f\"\\nCoherence Score: {coherence_score:.4f}\")\n",
    "        except ImportError:\n",
    "            print(\"\\nInstall gensim[complete] for coherence scoring.\")\n",
    "\n",
    "        return {\n",
    "            'model': lsa,\n",
    "            'dictionary': dictionary,\n",
    "            'corpus': corpus,\n",
    "            'topics': topics\n",
    "        }\n",
    "\n",
    "    def get_summary_stats(self):\n",
    "        total_words = sum(self.word_counts.values())\n",
    "        unique_words = len(self.word_counts)\n",
    "        avg_sentiment = np.mean(list(self.averaged_scores.values())) if self.averaged_scores else 0\n",
    "\n",
    "        return {\n",
    "            'total_posts': len(self.posts),\n",
    "            'total_comments': sum(len(post.get('comments', [])) for post in self.posts),\n",
    "            'total_words': total_words,\n",
    "            'unique_words': unique_words,\n",
    "            'words_above_threshold': len(self.get_filtered_word_frequencies()),\n",
    "            'average_sentiment': avg_sentiment,\n",
    "            'sentiment_std': np.std(list(self.averaged_scores.values())) if self.averaged_scores else 0\n",
    "        }\n",
    "\n",
    "    def get_individual_sentiments(self):\n",
    "        results = []\n",
    "        for canonical in self.aliases:\n",
    "            token = canonical.lower().replace(\" \", \"_\")\n",
    "            if token in self.word_counts:\n",
    "                sentiment = self.averaged_scores.get(token, 0)\n",
    "                count = self.word_counts[token]\n",
    "                std_dev = np.std([s for t, s in self.word_contexts[token]]) if self.word_contexts[token] else 0\n",
    "                results.append({\n",
    "                    'name': canonical,\n",
    "                    'token': token,\n",
    "                    'mention_count': count,\n",
    "                    'average_sentiment': sentiment,\n",
    "                    'sentiment_std_dev': std_dev\n",
    "                })\n",
    "\n",
    "        results.sort(key=lambda x: x['average_sentiment'], reverse=True)\n",
    "        return results\n",
    "\n",
    "    def print_analysis_summary(self):\n",
    "        stats = self.get_summary_stats()\n",
    "        extremes = self.get_sentiment_extremes(5)\n",
    "        common_words = self.get_most_common_words(10)\n",
    "\n",
    "        print(\"TEXT ANALYSIS SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Period: {self.period or 'All time'}\")\n",
    "        print(f\"Total Posts: {stats['total_posts']}\")\n",
    "        print(f\"Total Comments: {stats['total_comments']}\")\n",
    "        print(f\"Total Words: {stats['total_words']:,}\")\n",
    "        print(f\"Unique Words: {stats['unique_words']:,}\")\n",
    "        print(f\"Words Above Threshold of {self.min_word_freq}: {stats['words_above_threshold']:,}\")\n",
    "        print(f\"Average Sentiment: {stats['average_sentiment']:.3f}\")\n",
    "        print(f\"Sentiment Std Dev: {stats['sentiment_std']:.3f}\")\n",
    "\n",
    "        print(f\"\\nMOST COMMON WORDS:\")\n",
    "        for word, count in common_words:\n",
    "            sentiment = self.averaged_scores.get(word, 0)\n",
    "            print(f\"  {word}: {count} times (sentiment: {sentiment:.3f})\")\n",
    "\n",
    "        print(f\"\\nMOST POSITIVE WORDS:\")\n",
    "        for i, (word, sentiment) in enumerate(extremes['most_positive']):\n",
    "            count = self.word_counts[word]\n",
    "            print(f\"  {word}: {sentiment:.3f} ({count} times)\")\n",
    "            if i < 2:\n",
    "                self.explain_word_sentiment(word)\n",
    "\n",
    "        print(f\"\\nMOST NEGATIVE WORDS:\")\n",
    "        for i, (word, sentiment) in enumerate(reversed(extremes['most_negative'])):\n",
    "            count = self.word_counts[word]\n",
    "            print(f\"  {word}: {sentiment:.3f} ({count} times)\")\n",
    "            if i < 2:\n",
    "                self.explain_word_sentiment(word)\n",
    "\n",
    "    def get_top_tfidf_words(self, top_n=20):\n",
    "        documents = []\n",
    "\n",
    "        for post in self.posts:\n",
    "            if post.get(\"title\"):\n",
    "                documents.append(self._replace_aliases(post['title']))\n",
    "            for comment in post.get('comments', []):\n",
    "                if comment.get(\"body\"):\n",
    "                    documents.append(self._replace_aliases(comment['body']))\n",
    "\n",
    "        if not documents:\n",
    "            return []\n",
    "\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words=self.stop_words,\n",
    "            ngram_range=self.ngram_range,\n",
    "            token_pattern=r'\\b[a-z_]{3,}\\b',\n",
    "            lowercase=True\n",
    "        )\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "        mean_scores = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "        tfidf_scores = dict(zip(vectorizer.get_feature_names_out(), mean_scores))\n",
    "        top_tfidf = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        return top_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ea346",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "os.makedirs(\"graphs_analysis\", exist_ok=True)\n",
    "\n",
    "client = RedditClient()\n",
    "processor = Processor()\n",
    "all_posts = []\n",
    "\n",
    "# Uncomment to fetch new posts\n",
    "# client.fetch_all_posts()\n",
    "all_posts = client.get_all_posts()\n",
    "processed_posts = processor.process_posts(all_posts)\n",
    "\n",
    "posts_by_period = defaultdict(list)\n",
    "for post in processed_posts:\n",
    "    for period in post['periods']:\n",
    "        posts_by_period[period].append(post)\n",
    "\n",
    "period_graphs = {}\n",
    "sentiment_over_time = defaultdict(dict)\n",
    "\n",
    "for period, posts in posts_by_period.items():\n",
    "    print(f\"\\n=== Processing period: {period} | Posts: {len(posts)} ===\")\n",
    "\n",
    "    # Build and analyze graph\n",
    "    gm = GraphMaker(period)\n",
    "    gm.build_graph(posts)\n",
    "    gm.finalize_graph()\n",
    "    period_graphs[period] = gm\n",
    "    # gm.save_graph_info()\n",
    "    gm.export_edge_sentiment_analysis(f\"graphs_analysis/sentiment_edges_{period}.csv\")\n",
    "\n",
    "    # Text analysis\n",
    "    text_analysis = TextAnalysis(\n",
    "        posts=posts,\n",
    "        period=period,\n",
    "        aliases=CONFIG.ALIASES,\n",
    "        ngram_range=(1, 2)  # use unigrams and bigrams\n",
    "    )\n",
    "\n",
    "    text_analysis.process_all_text()\n",
    "    individual_stats = text_analysis.get_individual_sentiments()\n",
    "    text_analysis.generate_word_cloud()\n",
    "\n",
    "    # Save summary stats\n",
    "    summary = text_analysis.get_summary_stats()\n",
    "    with open(f\"outputs/summary_stats_{period}.csv\", \"w\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Metric\", \"Value\"])\n",
    "        for key, value in summary.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "    # Save most common words\n",
    "    common_words = text_analysis.get_most_common_words(n=50)\n",
    "    with open(f\"outputs/common_words_{period}.csv\", \"w\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Word\", \"Count\", \"Average Sentiment\"])\n",
    "        for word, count in common_words:\n",
    "            sentiment = text_analysis.averaged_scores.get(word, 0)\n",
    "            writer.writerow([word, count, round(sentiment, 4)])\n",
    "\n",
    "    # Save sentiment extremes\n",
    "    extremes = text_analysis.get_sentiment_extremes(n=10)\n",
    "    for sentiment_type, word_list in extremes.items():\n",
    "        with open(f\"outputs/{sentiment_type}_{period}.csv\", \"w\", newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Word\", \"Average Sentiment\", \"Count\"])\n",
    "            for word, sentiment in word_list:\n",
    "                count = text_analysis.word_counts[word]\n",
    "                writer.writerow([word, round(sentiment, 4), count])\n",
    "\n",
    "    # Save top TF-IDF n-grams\n",
    "    tfidf_words = text_analysis.get_top_tfidf_words(top_n=50)\n",
    "    with open(f\"outputs/tfidf_words_{period}.csv\", \"w\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Word\", \"TF-IDF Score\"])\n",
    "        writer.writerows([(word, round(score, 5)) for word, score in tfidf_words])\n",
    "\n",
    "    # Save individual entity sentiment stats\n",
    "    with open(f\"outputs/entity_sentiments_{period}.csv\", \"w\", newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            \"name\", \"token\", \"mention_count\", \"average_sentiment\", \"sentiment_std_dev\"\n",
    "        ])\n",
    "        writer.writeheader()\n",
    "        for stat in individual_stats:\n",
    "            writer.writerow(stat)\n",
    "\n",
    "    # Store sentiment by entity over time\n",
    "    for stat in individual_stats:\n",
    "        sentiment_over_time[stat['name']][period] = stat['average_sentiment']\n",
    "\n",
    "    # Visualize graph\n",
    "    gm.visualize()\n",
    "\n",
    "# Save overall sentiment evolution\n",
    "with open(\"outputs/sentiment_over_time.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    all_periods = sorted(posts_by_period.keys())\n",
    "    header = [\"Entity\"] + all_periods\n",
    "    writer.writerow(header)\n",
    "    for entity, sentiments in sentiment_over_time.items():\n",
    "        row = [entity] + [round(sentiments.get(period, 0), 4) for period in all_periods]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2d85c",
   "metadata": {},
   "source": [
    "## Musk - Pete connection thorughout the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6d60d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_co_mentions(posts, person1, person2, person3=None, aliases=None, period=None):\n",
    "    \"\"\"\n",
    "    Find co-mentions between two or more persons in a list of posts.\n",
    "    \"\"\"\n",
    "    person1_tokens = [person1.lower().replace(\" \", \"_\")]\n",
    "    person2_tokens = [person2.lower().replace(\" \", \"_\")]\n",
    "    if person3:\n",
    "        person3_tokens = [person3.lower().replace(\" \", \"_\")]\n",
    "\n",
    "    if aliases:\n",
    "        person1_tokens += [alias.lower().replace(\" \", \"_\") for alias in aliases.get(person1, [])]\n",
    "        person2_tokens += [alias.lower().replace(\" \", \"_\") for alias in aliases.get(person2, [])]\n",
    "        if person3:\n",
    "            person3_tokens += [alias.lower().replace(\" \", \"_\") for alias in aliases.get(person3, [])]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for post in posts:\n",
    "        # Check title\n",
    "        title = post.get(\"title\", \"\").lower()\n",
    "        if any(p1 in title for p1 in person1_tokens) and any(p2 in title for p2 in person2_tokens) and (not person3 or any(p3 in title for p3 in person3_tokens)):\n",
    "            results.append({\n",
    "                \"type\": \"title\",\n",
    "                \"text\": post[\"title\"],\n",
    "                \"created_utc\": post.get(\"created_utc\"),\n",
    "                \"score\": post.get(\"score\")\n",
    "            })\n",
    "\n",
    "        # Check comments\n",
    "        for comment in post.get(\"comments\", []):\n",
    "            body = comment.get(\"body\", \"\").lower()\n",
    "            if any(p1 in body for p1 in person1_tokens) and any(p2 in body for p2 in person2_tokens) and (not person3 or any(p3 in body for p3 in person3_tokens)):\n",
    "                results.append({\n",
    "                    \"type\": \"comment\",\n",
    "                    \"text\": comment[\"body\"],\n",
    "                    \"created_utc\": comment.get(\"created_utc\"),\n",
    "                    \"score\": comment.get(\"score\", 0),\n",
    "                    \"author\": comment.get(\"author\")\n",
    "                })\n",
    "\n",
    "    return [r[\"text\"] for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "def get_sentiment_scores(text_list: list[str], top_n=10, min_word_freq=3):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    word_scores = defaultdict(list)\n",
    "    word_counts = Counter()\n",
    "\n",
    "    for text in text_list:\n",
    "        if not text:\n",
    "            continue\n",
    "        tokens = re.findall(r'\\b[a-z]{3,}\\b', text.lower())\n",
    "        filtered_tokens = [w for w in tokens if w not in stopwords]\n",
    "\n",
    "        for word in set(filtered_tokens):\n",
    "            sentiment = sia.polarity_scores(word)['compound']\n",
    "            word_scores[word].append(sentiment)\n",
    "            word_counts[word] += filtered_tokens.count(word)\n",
    "\n",
    "    averaged_scores = {\n",
    "        word: np.mean(scores)\n",
    "        for word, scores in word_scores.items()\n",
    "        if word_counts[word] >= min_word_freq\n",
    "    }\n",
    "\n",
    "    sorted_positive = sorted(averaged_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return sorted_positive\n",
    "\n",
    "jim_elon_2022_2023 = find_co_mentions(processed_posts, \"Jim Cramer\", \"Elon Musk\", aliases=CONFIG.ALIASES, period=\"2022-2023\")\n",
    "biden_senders_before_2020 = find_co_mentions(processed_posts, \"Joe Biden\", \"Bernie Sanders\", aliases=CONFIG.ALIASES, period=\"<2020\")\n",
    "trump_sanders_before_2020 = find_co_mentions(processed_posts, \"Donald Trump\", \"Bernie Sanders\", aliases=CONFIG.ALIASES, period=\"<2020\")\n",
    "trump_biden_before_2020 = find_co_mentions(processed_posts, \"Donald Trump\", \"Joe Biden\", aliases=CONFIG.ALIASES, period=\"<2020\")\n",
    "\n",
    "\n",
    "threesome = [biden_senders_before_2020, trump_sanders_before_2020, trump_biden_before_2020, trump_pete_20_21, trump_pete_24_25] # 20 sentences\n",
    "\n",
    "def get_top_tfidf_words(documents: list[str], top_n=20):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=list(stopwords) + [\"elon\", \"musk\", \"jim\", \"cramer\", \"biden\", \"sanders\", \"trump\", \"bernie\"],\n",
    "        ngram_range=(1,1),\n",
    "        token_pattern=r'\\b[a-z_]{3,}\\b',\n",
    "        lowercase=True\n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    mean_scores = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "    tfidf_scores = dict(zip(vectorizer.get_feature_names_out(), mean_scores))\n",
    "    top_tfidf = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return top_tfidf\n",
    "\n",
    "# Join each group into a single document\n",
    "threesome_docs = [\" \".join(group) for group in threesome]\n",
    "\n",
    "# Now run\n",
    "print(get_top_tfidf_words(threesome_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114090c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Only run these once to download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords and punctuation setup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_texts(raw_texts):\n",
    "    texts = []\n",
    "    for doc in raw_texts:\n",
    "        tokens = word_tokenize(doc.lower().translate(punct_table))\n",
    "        filtered = [token for token in tokens if token not in stop_words and len(token) > 3 and token not in names]\n",
    "        if filtered:\n",
    "            texts.append(filtered)\n",
    "    return texts\n",
    "\n",
    "# LDA function\n",
    "def run_lda(texts, num_topics=2):\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    if not corpus:\n",
    "        print(\"No valid documents for LDA after filtering.\")\n",
    "        return None\n",
    "\n",
    "    lda = models.LdaModel(corpus, id2word=dictionary, num_topics=num_topics, random_state=42)\n",
    "    topics = lda.print_topics(num_topics=num_topics, num_words=8)\n",
    "\n",
    "    print(f\"\\nLDA Topics (num_topics={num_topics}):\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, topic in enumerate(topics):\n",
    "        print(f\"Topic {i}: {topic[1]}\")\n",
    "\n",
    "    try:\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=lda, texts=texts, dictionary=dictionary, coherence='c_v'\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        print(f\"\\nCoherence Score: {coherence_score:.4f}\")\n",
    "    except ImportError:\n",
    "        print(\"\\nInstall gensim[complete] for coherence scoring.\")\n",
    "\n",
    "    return {\n",
    "        'model': lda,\n",
    "        'dictionary': dictionary,\n",
    "        'corpus': corpus,\n",
    "        'topics': topics\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trump_pete_20_21 = find_co_mentions(processed_posts, \"Donald Trump\", \"Pete Buttigieg\", aliases=CONFIG.ALIASES, period=\"2020-2021\")\n",
    "trump_pete_24_25 = find_co_mentions(processed_posts, \"Donald Trump\", \"Pete Buttigieg\", aliases=CONFIG.ALIASES, period=\"2024-2025\")\n",
    "\n",
    "# texts_20_21 = preprocess_texts(trump_pete_20_21)\n",
    "# lda_output = run_lda(texts_20_21, num_topics=2)\n",
    "# 2024-2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def histogram_of_words_sentiment(words: list[str]):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    categories = {\"positive\": 0, \"neutral\": 0, \"negative\": 0}\n",
    "\n",
    "    for word in words:\n",
    "        score = sia.polarity_scores(word)['compound']\n",
    "        if score >= CONFIG.SENTIMENT_THRESHOLD:\n",
    "            categories[\"positive\"] += 1\n",
    "        elif score <= -CONFIG.SENTIMENT_THRESHOLD:\n",
    "            categories[\"negative\"] += 1\n",
    "        else:\n",
    "            categories[\"neutral\"] += 1\n",
    "\n",
    "    labels = list(categories.keys())\n",
    "    counts = list(categories.values())\n",
    "\n",
    "    plt.bar(labels, counts, color=['green', 'gray', 'red'])\n",
    "    plt.title(\"Sentiment Distribution of Words\")\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Number of Words\")\n",
    "    plt.show()\n",
    "\n",
    "    return categories\n",
    "\n",
    "\n",
    "print(histogram_of_words_sentiment(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8edc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = [post[\"title\"] + \" \" + \" \".join([comment[\"body\"] for comment in post[\"comments\"]]) for post in processed_posts]\n",
    "processed_sentences = preprocess_texts(all_sentences)\n",
    "lda_output = run_lda(processed_sentences, num_topics=2)\n",
    "print(lda_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".final_project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
